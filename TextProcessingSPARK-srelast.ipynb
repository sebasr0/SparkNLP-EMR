{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33eff85f",
   "metadata": {},
   "source": [
    "# Text Processing:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd84853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:31:19.220813Z",
     "iopub.status.busy": "2024-11-07T22:31:19.220475Z",
     "iopub.status.idle": "2024-11-07T22:32:16.624613Z",
     "shell.execute_reply": "2024-11-07T22:32:16.623915Z",
     "shell.execute_reply.started": "2024-11-07T22:31:19.220786Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43fd5f488fa496d912f4dc89ed36a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2</td><td>application_1731014642264_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-46-18.ec2.internal:20888/proxy/application_1731014642264_0003/\" class=\"emr-proxy-link j-35C4AGJSJAIAH application_1731014642264_0003\" emr-resource=\"j-35C4AGJSJAIAH\n",
       "\" application-id=\"application_1731014642264_0003\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-32-237.ec2.internal:8042/node/containerlogs/container_1731014642264_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take."
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6658c104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:16.625819Z",
     "iopub.status.busy": "2024-11-07T22:32:16.625638Z",
     "iopub.status.idle": "2024-11-07T22:32:16.678133Z",
     "shell.execute_reply": "2024-11-07T22:32:16.677538Z",
     "shell.execute_reply.started": "2024-11-07T22:32:16.625796Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4b3af274224385a14b3b9a1445c023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'9486M'"
     ]
    }
   ],
   "source": [
    "sc._conf.get('spark.executor.memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f5953",
   "metadata": {},
   "source": [
    "## Librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645cbe67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:16.679609Z",
     "iopub.status.busy": "2024-11-07T22:32:16.679436Z",
     "iopub.status.idle": "2024-11-07T22:32:16.728448Z",
     "shell.execute_reply": "2024-11-07T22:32:16.727895Z",
     "shell.execute_reply.started": "2024-11-07T22:32:16.679589Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03af28ff9f814239a6051fc0213a4076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PySpark SQL Modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when, trim, split, udf, expr\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "# PySpark ML Modules\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Other Libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb680a",
   "metadata": {},
   "source": [
    "## Lectura de Datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd09b455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:16.729724Z",
     "iopub.status.busy": "2024-11-07T22:32:16.729566Z",
     "iopub.status.idle": "2024-11-07T22:32:16.777635Z",
     "shell.execute_reply": "2024-11-07T22:32:16.777103Z",
     "shell.execute_reply.started": "2024-11-07T22:32:16.729704Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d58afea59b84bf69b4ddcd9cce42b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ruta al archivo JSON\n",
    "file_path = 's3://bucketspark14/complaints.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adabfd34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:16.778745Z",
     "iopub.status.busy": "2024-11-07T22:32:16.778583Z",
     "iopub.status.idle": "2024-11-07T22:32:28.081369Z",
     "shell.execute_reply": "2024-11-07T22:32:28.080720Z",
     "shell.execute_reply.started": "2024-11-07T22:32:16.778725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921cb40570944bbbb5bea52683ffd3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar archivo JSON en Spark DataFrame\n",
    "df_tickets = spark.read.json(file_path)\n",
    "\n",
    "# Desanidar las columnas dentro de _source\n",
    "df_tickets_flat = df_tickets.select(\n",
    "    \"_id\", \"_index\", \"_score\",\n",
    "    col(\"_source.tags\").alias(\"tags\"),\n",
    "    col(\"_source.zip_code\").alias(\"zip_code\"),\n",
    "    col(\"_source.complaint_id\").alias(\"complaint_id\"),\n",
    "    col(\"_source.issue\").alias(\"issue\"),\n",
    "    col(\"_source.date_received\").alias(\"date_received\"),\n",
    "    col(\"_source.state\").alias(\"state\"),\n",
    "    col(\"_source.consumer_disputed\").alias(\"consumer_disputed\"),\n",
    "    col(\"_source.product\").alias(\"product\"),\n",
    "    col(\"_source.company_response\").alias(\"company_response\"),\n",
    "    col(\"_source.company\").alias(\"company\"),\n",
    "    col(\"_source.submitted_via\").alias(\"submitted_via\"),\n",
    "    col(\"_source.date_sent_to_company\").alias(\"date_sent_to_company\"),\n",
    "    col(\"_source.company_public_response\").alias(\"company_public_response\"),\n",
    "    col(\"_source.sub_product\").alias(\"sub_product\"),\n",
    "    col(\"_source.timely\").alias(\"timely\"),\n",
    "    col(\"_source.complaint_what_happened\").alias(\"complaint_what_happened\"),\n",
    "    col(\"_source.sub_issue\").alias(\"sub_issue\"),\n",
    "    col(\"_source.consumer_consent_provided\").alias(\"consumer_consent_provided\"),\n",
    "    \"_type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0623d",
   "metadata": {},
   "source": [
    "## Limpieza de Datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5b35ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:28.082866Z",
     "iopub.status.busy": "2024-11-07T22:32:28.082625Z",
     "iopub.status.idle": "2024-11-07T22:32:31.360972Z",
     "shell.execute_reply": "2024-11-07T22:32:31.360263Z",
     "shell.execute_reply.started": "2024-11-07T22:32:28.082833Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfdec0f9aea4c02ae19746b91f891f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+\n",
      "|complaint_what_happened|            category|\n",
      "+-----------------------+--------------------+\n",
      "|   Good morning my n...|Debt collection+C...|\n",
      "|   I upgraded my XXX...|Credit card or pr...|\n",
      "|   Chase Card was re...|Credit reporting,...|\n",
      "|   On XX/XX/2018, wh...|Credit reporting,...|\n",
      "|   my grand son give...|Checking or savin...|\n",
      "|   Can you please re...|Credit reporting,...|\n",
      "|   With out notice J...|Checking or savin...|\n",
      "|   During the summer...|Vehicle loan or l...|\n",
      "|   On XXXX XX/XX/201...|Money transfer, v...|\n",
      "|   I have a Chase cr...|Credit card or pr...|\n",
      "|   mishandling of th...|Vehicle loan or l...|\n",
      "|   I have reached ou...|Credit reporting,...|\n",
      "|   I opened an accou...|Checking or savin...|\n",
      "|   To whom it may co...|Checking or savin...|\n",
      "|   My chase amazon c...|Credit card or pr...|\n",
      "|   I opened the savi...|Checking or savin...|\n",
      "|   XXXX XXXX a sofa,...|Checking or savin...|\n",
      "|   My card went miss...|Checking or savin...|\n",
      "|   Chase sent me an ...|Credit card or pr...|\n",
      "|   I made a purchase...|Checking or savin...|\n",
      "+-----------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Seleccionar la columnas\n",
    "df_filtered = df_tickets_flat.select('complaint_what_happened','product','sub_product')\n",
    "\n",
    "# Concatenar las columnas 'category' y 'sub_category' con un '+' en medio\n",
    "df_filtered = df_filtered.withColumn('category', F.concat_ws('+', F.col('product'), F.col('sub_product')))\n",
    "\n",
    "# Eliminar la columna 'sub_category'\n",
    "df_filtered = df_filtered.drop('product','sub_product')\n",
    "\n",
    "# Reemplazar cadenas vacías por nulos\n",
    "df_filtered = df_filtered.withColumn(\"complaint_what_happened\", when(trim(col(\"complaint_what_happened\")) == \"\", lit(None)).otherwise(col(\"complaint_what_happened\")))        \n",
    "\n",
    "# Eliminar filas con nulos en columnas especificadas\n",
    "df_filtered = df_filtered.dropna(subset=[\"complaint_what_happened\", \"category\"])\n",
    "\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42dd5085",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:31.362335Z",
     "iopub.status.busy": "2024-11-07T22:32:31.362086Z",
     "iopub.status.idle": "2024-11-07T22:32:33.637888Z",
     "shell.execute_reply": "2024-11-07T22:32:33.637018Z",
     "shell.execute_reply.started": "2024-11-07T22:32:31.362302Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df338182fe1d44b39aa168a46ab5a8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+\n",
      "|complaint_what_happened|            category|\n",
      "+-----------------------+--------------------+\n",
      "|   good morning my n...|Debt collection+C...|\n",
      "|   i upgraded my xxx...|Credit card or pr...|\n",
      "|   chase card was re...|Credit reporting,...|\n",
      "|   on  while trying ...|Credit reporting,...|\n",
      "|   my grand son give...|Checking or savin...|\n",
      "|   can you please re...|Credit reporting,...|\n",
      "|   with out notice j...|Checking or savin...|\n",
      "|   during the summer...|Vehicle loan or l...|\n",
      "|   on xxxx  i made a...|Money transfer, v...|\n",
      "|   i have a chase cr...|Credit card or pr...|\n",
      "|   mishandling of th...|Vehicle loan or l...|\n",
      "|   i have reached ou...|Credit reporting,...|\n",
      "|   i opened an accou...|Checking or savin...|\n",
      "|   to whom it may co...|Checking or savin...|\n",
      "|   my chase amazon c...|Credit card or pr...|\n",
      "|   i opened the savi...|Checking or savin...|\n",
      "|   xxxx xxxx a sofa ...|Checking or savin...|\n",
      "|   my card went miss...|Checking or savin...|\n",
      "|   chase sent me an ...|Credit card or pr...|\n",
      "|   i made a purchase...|Checking or savin...|\n",
      "+-----------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Definir una UDF para aplicar todas las transformaciones en Spark\n",
    "def clean_text_spark(df, text_column):\n",
    "    # Convertir el texto a minúsculas\n",
    "    df = df.withColumn(text_column, F.lower(F.col(text_column)))\n",
    "    \n",
    "    # Eliminar texto en corchetes []\n",
    "    df = df.withColumn(text_column, F.regexp_replace(F.col(text_column), r'\\[.*?\\]', ''))\n",
    "    \n",
    "    # Eliminar puntuación\n",
    "    df = df.withColumn(text_column, F.regexp_replace(F.col(text_column), r'[^\\w\\s]', ''))\n",
    "    \n",
    "    # Eliminar palabras que contienen números\n",
    "    df = df.withColumn(text_column, F.regexp_replace(F.col(text_column), r'\\b\\w*\\d\\w*\\b', ''))\n",
    "    \n",
    "    # Eliminar espacios en blanco adicionales\n",
    "    df = df.withColumn(text_column, F.trim(F.col(text_column)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicar la función de limpieza\n",
    "df_cleanedxx = clean_text_spark(df_filtered, 'complaint_what_happened')\n",
    "\n",
    "# Mostrar el resultado limpio\n",
    "df_cleanedxx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230395fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:33.639629Z",
     "iopub.status.busy": "2024-11-07T22:32:33.639468Z",
     "iopub.status.idle": "2024-11-07T22:32:39.044939Z",
     "shell.execute_reply": "2024-11-07T22:32:39.044067Z",
     "shell.execute_reply.started": "2024-11-07T22:32:33.639610Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1786e2a43414d44a85805cbb08ab38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------------------+\n",
      "|complaint_what_happened|            category|\n",
      "+-----------------------+--------------------+\n",
      "|   good morning my n...|Debt collection+C...|\n",
      "|   i upgraded mycard...|Credit card or pr...|\n",
      "|   chase card was re...|Credit reporting,...|\n",
      "|   on  while trying ...|Credit reporting,...|\n",
      "|   my grand son give...|Checking or savin...|\n",
      "|   can you please re...|Credit reporting,...|\n",
      "|   with out notice j...|Checking or savin...|\n",
      "|   during the summer...|Vehicle loan or l...|\n",
      "|   oni made a  payme...|Money transfer, v...|\n",
      "|   i have a chase cr...|Credit card or pr...|\n",
      "|   mishandling of th...|Vehicle loan or l...|\n",
      "|   i have reached ou...|Credit reporting,...|\n",
      "|   i opened an accou...|Checking or savin...|\n",
      "|   to whom it may co...|Checking or savin...|\n",
      "|   my chase amazon c...|Credit card or pr...|\n",
      "|   i opened the savi...|Checking or savin...|\n",
      "|   a sofa love seat ...|Checking or savin...|\n",
      "|   my card went miss...|Checking or savin...|\n",
      "|   chase sent me an ...|Credit card or pr...|\n",
      "|   i made a purchase...|Checking or savin...|\n",
      "+-----------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# Define a UDF to remove \"xxxx\" and any extra spaces left by the removal\n",
    "def remove_xxxx(text):\n",
    "    # Remove \"xxxx\" and any spaces left behind\n",
    "    return re.sub(r\"\\s*xxxx\\s*\", \"\", text)\n",
    "\n",
    "# Register the UDF with Spark\n",
    "remove_xxxx_udf = udf(remove_xxxx, StringType())\n",
    "\n",
    "# Apply the UDF to the 'complaint_what_happened' column\n",
    "df_cleaned = df_cleanedxx.withColumn(\"complaint_what_happened\", remove_xxxx_udf(\"complaint_what_happened\"))\n",
    "\n",
    "# Verify the column after cleaning\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cfa92",
   "metadata": {},
   "source": [
    "## Lemmatization, Stopwords, Pos Tagging, CountVectorizer.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b0c946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:32:39.046759Z",
     "iopub.status.busy": "2024-11-07T22:32:39.046520Z",
     "iopub.status.idle": "2024-11-07T22:33:06.408582Z",
     "shell.execute_reply": "2024-11-07T22:33:06.407891Z",
     "shell.execute_reply.started": "2024-11-07T22:32:39.046724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cddd0d3b281458686b829388f5f7219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "+--------------------+--------------------+\n",
      "|         noun_tokens|                 pos|\n",
      "+--------------------+--------------------+\n",
      "|[morning, name, i...|[{pos, 0, 3, JJ, ...|\n",
      "|[mycard, agent, u...|[{pos, 2, 9, JJ, ...|\n",
      "|[chase, card, app...|[{pos, 0, 4, NN, ...|\n",
      "|[book, aticket, o...|[{pos, 10, 15, VB...|\n",
      "|[son, check, depo...|[{pos, 3, 7, JJ, ...|\n",
      "|           [inquiry]|[{pos, 8, 13, VB,...|\n",
      "|[notice, jp, morg...|[{pos, 9, 14, NN,...|\n",
      "|[summer, experien...|[{pos, 11, 16, NN...|\n",
      "|[oni, payment, on...|[{pos, 0, 2, NN, ...|\n",
      "|[chase, credit, c...|[{pos, 9, 13, NN,...|\n",
      "|[account, chase, ...|[{pos, 0, 10, VBG...|\n",
      "|[attempt, inquiry...|[{pos, 7, 13, VBD...|\n",
      "|[account, chase, ...|[{pos, 2, 7, VBD,...|\n",
      "|[bank, alert, bal...|[{pos, 11, 13, MD...|\n",
      "|[chase, amazon, c...|[{pos, 3, 7, NN, ...|\n",
      "|[account, bonus, ...|[{pos, 2, 7, VBD,...|\n",
      "|[sofa, love, seat...|[{pos, 2, 5, NN, ...|\n",
      "|[card, didnt, rea...|[{pos, 3, 6, NN, ...|\n",
      "|[chase, email, to...|[{pos, 0, 4, NN, ...|\n",
      "|[purchase, withon...|[{pos, 2, 5, VBN,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, LemmatizerModel, PerceptronModel, StopWordsCleaner\n",
    "\n",
    "# Iniciar sesión de Spark con Spark NLP\n",
    "# spark = sparknlp.start()\n",
    "\n",
    "# Document Assembler\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"complaint_what_happened\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Stop Words Remover\n",
    "stop_words_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"cleanTokens\") \\\n",
    "    .setCaseSensitive(False)\n",
    "\n",
    "# POS Tagger BERT\n",
    "pos_tagger = PerceptronModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"pos\")\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "# Finisher to extract results from annotations\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"lemma\", \"pos\"]) \\\n",
    "    .setOutputCols([\"finished_lemma\", \"finished_pos\"]) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "# Custom Transformer to remove pronouns and extract nouns\n",
    "class NounExtractor(Transformer):\n",
    "    def __init__(self, inputColPOS=None, inputColLemma=None, outputCol=None):\n",
    "        super(NounExtractor, self).__init__()\n",
    "        self.inputColPOS = inputColPOS\n",
    "        self.inputColLemma = inputColLemma\n",
    "        self.outputCol = outputCol\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        def extract_nouns(pos_tags, lemmas):\n",
    "            #noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "            noun_tags = ['NN']\n",
    "            return [lemma for pos, lemma in zip(pos_tags, lemmas) if pos in noun_tags]\n",
    "        \n",
    "        extract_nouns_udf = udf(extract_nouns, ArrayType(StringType()))\n",
    "        return dataset.withColumn(self.outputCol, extract_nouns_udf(col(self.inputColPOS), col(self.inputColLemma)))\n",
    "\n",
    "# Instantiate custom transformer\n",
    "noun_extractor = NounExtractor(inputColPOS=\"finished_pos\", inputColLemma=\"finished_lemma\", outputCol=\"noun_tokens\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    stop_words_cleaner,\n",
    "    pos_tagger,\n",
    "    lemmatizer,\n",
    "    finisher,\n",
    "    noun_extractor\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "pipeline_model = pipeline.fit(df_cleaned)\n",
    "df_final = pipeline_model.transform(df_cleaned)\n",
    "df_final.select(\"noun_tokens\", \"pos\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bf791-573c-4f2e-a095-8b37651c596d",
   "metadata": {},
   "source": [
    "## NGRAM COUNTVECTORIZER-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9579e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:33:06.409950Z",
     "iopub.status.busy": "2024-11-07T22:33:06.409760Z",
     "iopub.status.idle": "2024-11-07T22:37:20.413205Z",
     "shell.execute_reply": "2024-11-07T22:37:20.412560Z",
     "shell.execute_reply.started": "2024-11-07T22:33:06.409928Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96af3a70681744a5b2e6e1085aa24b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         noun_tokens|      tfidf_features|\n",
      "+--------------------+--------------------+\n",
      "|[morning, name, i...|(14263,[0,3,10,38...|\n",
      "|[mycard, agent, u...|(14263,[10,40,75,...|\n",
      "|[chase, card, app...|(14263,[0,2,4,76,...|\n",
      "|[book, aticket, o...|(14263,[0,1,2,3,4...|\n",
      "|[son, check, depo...|(14263,[0,1,3,6,7...|\n",
      "|           [inquiry]|(14263,[128],[3.2...|\n",
      "|[notice, jp, morg...|(14263,[0,1,3,4,6...|\n",
      "|[summer, experien...|(14263,[0,5,9,10,...|\n",
      "|[oni, payment, on...|(14263,[0,3,5,7,8...|\n",
      "|[chase, credit, c...|(14263,[0,2,4,16,...|\n",
      "|[account, chase, ...|(14263,[0,1,139],...|\n",
      "|[attempt, inquiry...|(14263,[0,1,2,3,1...|\n",
      "|[account, chase, ...|(14263,[0,1,3,32,...|\n",
      "|[bank, alert, bal...|(14263,[0,3,7,11,...|\n",
      "|[chase, amazon, c...|(14263,[0,1,4,11,...|\n",
      "|[account, bonus, ...|(14263,[1,6,38,45...|\n",
      "|[sofa, love, seat...|(14263,[3,4,7,26,...|\n",
      "|[card, didnt, rea...|(14263,[1,3,4,7,3...|\n",
      "|[chase, email, to...|(14263,[0,1,7,8,1...|\n",
      "|[purchase, withon...|(14263,[0,1,2,3,4...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import array_union, col\n",
    "\n",
    "# Generar bigramas y trigramas a partir de noun_tokens\n",
    "ngram2 = NGram(n=2, inputCol=\"noun_tokens\", outputCol=\"bigrams\")\n",
    "ngram3 = NGram(n=3, inputCol=\"noun_tokens\", outputCol=\"trigrams\")\n",
    "\n",
    "df_bigrams = ngram2.transform(df_final)\n",
    "df_trigrams = ngram3.transform(df_bigrams)\n",
    "\n",
    "# Combinar palabras individuales, bigramas y trigramas en una sola columna llamada all_ngrams\n",
    "df_combined = df_trigrams.withColumn(\n",
    "    \"all_ngrams\", array_union(array_union(col(\"noun_tokens\"), col(\"bigrams\")), col(\"trigrams\"))\n",
    ")\n",
    "\n",
    "# Aplicar CountVectorizer para obtener la matriz de frecuencias de términos en noun_tokens\n",
    "count_vectorizer = CountVectorizer(inputCol=\"noun_tokens\", outputCol=\"tf_features\", minDF=2, maxDF=0.95)\n",
    "cv_model = count_vectorizer.fit(df_final)\n",
    "df_tf = cv_model.transform(df_final)\n",
    "\n",
    "# Aplicar IDF para obtener los valores de TF-IDF\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(df_tf)\n",
    "df_tfidf = idf_model.transform(df_tf)\n",
    "\n",
    "# Mostrar los resultados\n",
    "df_tfidf.select(\"noun_tokens\", \"tfidf_features\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107e83b",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b588b8-c46d-4887-ada3-1650fec62d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fcc45c7-05a2-47a8-9619-00dcdf2b2f4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:37:20.414991Z",
     "iopub.status.busy": "2024-11-07T22:37:20.414756Z",
     "iopub.status.idle": "2024-11-07T22:39:34.142913Z",
     "shell.execute_reply": "2024-11-07T22:39:34.107090Z",
     "shell.execute_reply.started": "2024-11-07T22:37:20.414957Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f09322cb50419686e75cc5788e2f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import NGram, CountVectorizer, IDF, Tokenizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.functions import col, array_union, expr, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import numpy as np\n",
    "\n",
    "# Crear sesión de Spark (si aún no está creada)\n",
    "# spark = SparkSession.builder.appName(\"LDA Example\").getOrCreate()\n",
    "\n",
    "# Aplicar LDA para encontrar patrones latentes en las PQRS\n",
    "lda = LDA(k=5, seed=1, maxIter=25, featuresCol=\"tfidf_features\")\n",
    "lda_model = lda.fit(df_tfidf)\n",
    "\n",
    "# Transformar los datos para asignar cada transacción a un tema\n",
    "df_transformed = lda_model.transform(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576e90a8-8a47-4fa9-9fc6-3b86cda8cb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:39:34.149937Z",
     "iopub.status.busy": "2024-11-07T22:39:34.149437Z",
     "iopub.status.idle": "2024-11-07T22:39:37.681928Z",
     "shell.execute_reply": "2024-11-07T22:39:37.677422Z",
     "shell.execute_reply.started": "2024-11-07T22:39:34.149912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d15cb043ee4474bfad8714bb22bb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|   topicDistribution|tema_predominante|\n",
      "+--------------------+-----------------+\n",
      "|[0.00262342188206...|                4|\n",
      "|[0.00241075125210...|                4|\n",
      "|[0.00730151254814...|                1|\n",
      "|[0.00118021653137...|                4|\n",
      "|[0.00193692144151...|                1|\n",
      "|[0.03911115134350...|                1|\n",
      "|[7.76745246158478...|                4|\n",
      "|[0.16822794859553...|                4|\n",
      "|[4.61600810463970...|                1|\n",
      "|[0.01468848764267...|                1|\n",
      "|[0.03256511554981...|                4|\n",
      "|[0.00206036935529...|                1|\n",
      "|[0.00366844232673...|                2|\n",
      "|[0.00345387375760...|                1|\n",
      "|[9.07194327753448...|                1|\n",
      "|[0.00303454248669...|                2|\n",
      "|[0.00362646637784...|                1|\n",
      "|[0.00978145144809...|                1|\n",
      "|[0.19425021867640...|                4|\n",
      "|[0.51561641455677...|                0|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tema 0:\n",
      "T?rminos predominantes: ['mortgage', 'loan', 'debt', 'credit', 'bank', 'payment', 'check', 'home', 'hotel', 'morgan']\n",
      "Pesos de t?rminos: [0.009866176612402107, 0.007279181078866165, 0.007148447845675159, 0.006929485652488284, 0.005424781897700119, 0.005374149467611587, 0.005278575935335641, 0.005156634585070094, 0.00499600562339454, 0.004405078836054013]\n",
      "\n",
      "\n",
      "Tema 1:\n",
      "T?rminos predominantes: ['account', 'card', 'bank', 'check', 'fraud', 'credit', 'dispute', 'transaction', 'claim', 'information']\n",
      "Pesos de t?rminos: [0.009953309450870161, 0.008798167667089298, 0.00798980578936266, 0.0077888459281693165, 0.007712832824998014, 0.007179872203966903, 0.006576782991298603, 0.006558205235801249, 0.006487259607023434, 0.006356611194721692]\n",
      "\n",
      "\n",
      "Tema 2:\n",
      "T?rminos predominantes: ['bonus', 'account', 'coupon', 'promotion', 'loan', 'offer', 'interest', 'seller', 'balance', 'check']\n",
      "Pesos de t?rminos: [0.01148497627038571, 0.006100786627083324, 0.005132041012611573, 0.004866229828190595, 0.004744708534273731, 0.004730832283942312, 0.004567906401075008, 0.004559810064408942, 0.004505508547763566, 0.00436549769722363]\n",
      "\n",
      "\n",
      "Tema 3:\n",
      "T?rminos predominantes: ['atm', 'card', 'check', 'account', 'money', 'bank', 'deposit', 'fee', 'cash', 'credit']\n",
      "Pesos de t?rminos: [0.01624964275740035, 0.01239517961014743, 0.008698614796870676, 0.0076827493240379085, 0.007633555643693863, 0.007495616431377222, 0.006761364512774517, 0.006718642227204222, 0.005412190321238309, 0.005371889405309571]\n",
      "\n",
      "\n",
      "Tema 4:\n",
      "T?rminos predominantes: ['loan', 'payment', 'mortgage', 'interest', 'credit', 'modification', 'balance', 'home', 'card', 'account']\n",
      "Pesos de t?rminos: [0.011175537585362211, 0.010212758291354106, 0.008549272837005368, 0.008254429564602709, 0.006946493284292024, 0.006314513555108793, 0.005890211618994372, 0.005652842447528718, 0.005467057301358557, 0.005148230404913978]"
     ]
    }
   ],
   "source": [
    "def get_predominant_topic(topic_distribution):\n",
    "    return int(np.argmax(topic_distribution))\n",
    "\n",
    "# Registrar la función UDF para obtener el tema predominante\n",
    "get_predominant_topic_udf = udf(get_predominant_topic, IntegerType())\n",
    "\n",
    "# Agregar columna para el tema predominante y mantener la distribución de temas\n",
    "df_results = df_transformed.withColumn(\"tema_predominante\", get_predominant_topic_udf(\"topicDistribution\"))\n",
    "\n",
    "# Mostrar el tema predominante y la distribución de temas para cada documento\n",
    "df_results.select(\"topicDistribution\", \"tema_predominante\").show()\n",
    "\n",
    "# Obtener los términos predominantes de cada tema\n",
    "topics = lda_model.describeTopics(10)  # Obtiene los 10 términos principales por tema\n",
    "\n",
    "# Extraer los términos del vocabulario para interpretar cada tema\n",
    "vocab = cv_model.vocabulary  # Supone que 'cv_model' es el CountVectorizer ajustado anteriormente\n",
    "\n",
    "# Mostrar términos predominantes y distribución de temas para cada tema\n",
    "for topic in range(5):  # Itera sobre el número de temas\n",
    "    term_indices = topics.where(col(\"topic\") == topic).select(\"termIndices\").head()[0]\n",
    "    term_weights = topics.where(col(\"topic\") == topic).select(\"termWeights\").head()[0]\n",
    "    terms = [vocab[idx] for idx in term_indices]\n",
    "    print(f\"Tema {topic}:\")\n",
    "    print(\"Términos predominantes:\", terms)\n",
    "    print(\"Pesos de términos:\", term_weights)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70028bab-40aa-4c50-b17b-d3b5336ae748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:39:37.684893Z",
     "iopub.status.busy": "2024-11-07T22:39:37.684363Z",
     "iopub.status.idle": "2024-11-07T22:41:43.403172Z",
     "shell.execute_reply": "2024-11-07T22:41:43.402561Z",
     "shell.execute_reply.started": "2024-11-07T22:39:37.684852Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8260a9fce9461d8dc756a9dee13a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|tema_predominante|count|\n",
      "+-----------------+-----+\n",
      "|                0| 2995|\n",
      "|                1| 8681|\n",
      "|                2| 1302|\n",
      "|                3| 2022|\n",
      "|                4| 6072|\n",
      "+-----------------+-----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Agrupar por `tema_predominante` y contar la cantidad de documentos en cada tema\n",
    "df_topic_counts = df_results.groupBy(\"tema_predominante\").agg(F.count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Mostrar la distribución de temas predominantes\n",
    "df_topic_counts.orderBy(\"tema_predominante\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ea6db-e902-4713-9a58-c7394382232d",
   "metadata": {},
   "source": [
    "## Modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf925a3f-66d1-4d5c-ab40-14afd70b63a6",
   "metadata": {},
   "source": [
    "### Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00671733-4a1f-41e1-aead-1aba87b00343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:35:18.034559Z",
     "iopub.status.busy": "2024-11-07T23:35:18.034312Z",
     "iopub.status.idle": "2024-11-07T23:45:53.221125Z",
     "shell.execute_reply": "2024-11-07T23:45:53.220510Z",
     "shell.execute_reply.started": "2024-11-07T23:35:18.034528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cdedd1f3bf4c9a9dc37607e50af907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy = 0.7203836930455636\n",
      "Test Set F1 Score = 0.7256315794001083\n",
      "Test Set Recall = 0.7203836930455635"
     ]
    }
   ],
   "source": [
    "## Logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Assume df_results is the DataFrame containing 'tfidf_features' and 'tema_predominante'\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = df_results.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='tfidf_features', labelCol='tema_predominante', maxIter=20)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using MulticlassClassificationEvaluator\n",
    "# Accuracy\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(f\"Test Set Accuracy = {accuracy}\")\n",
    "\n",
    "# F1 Score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "print(f\"Test Set F1 Score = {f1}\")\n",
    "\n",
    "# Recall\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "print(f\"Test Set Recall = {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018092e-5b37-4a3f-911b-70693060f686",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a57beff-0029-4b90-bcc6-092a46627301",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T22:55:41.154856Z",
     "iopub.status.busy": "2024-11-07T22:55:41.154633Z",
     "iopub.status.idle": "2024-11-07T23:13:09.679956Z",
     "shell.execute_reply": "2024-11-07T23:13:09.679233Z",
     "shell.execute_reply.started": "2024-11-07T22:55:41.154832Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf9676664b46308ea2480de16271b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del conjunto de prueba (Decision Tree) = 0.5162270183852917\n",
      "Puntaje F1 del conjunto de prueba (Decision Tree) = 0.42286866429605396\n",
      "Recall del conjunto de prueba (Decision Tree) = 0.5162270183852918"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Crear el modelo Decision Tree\n",
    "dt = DecisionTreeClassifier(featuresCol='tfidf_features', labelCol='tema_predominante', maxDepth=5, seed=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Realizar predicciones en los datos de prueba\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo usando MulticlassClassificationEvaluator\n",
    "\n",
    "# Exactitud (Accuracy)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = evaluator_accuracy.evaluate(dt_predictions)\n",
    "print(f\"Exactitud del conjunto de prueba (Decision Tree) = {dt_accuracy}\")\n",
    "\n",
    "# Puntaje F1\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "dt_f1 = evaluator_f1.evaluate(dt_predictions)\n",
    "print(f\"Puntaje F1 del conjunto de prueba (Decision Tree) = {dt_f1}\")\n",
    "\n",
    "# Recall\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "dt_recall = evaluator_recall.evaluate(dt_predictions)\n",
    "print(f\"Recall del conjunto de prueba (Decision Tree) = {dt_recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b9f10-a066-40fa-96e4-711a9f285ebe",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77868ac0-98f7-4e27-a5a9-7375ebd724ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:13:09.683209Z",
     "iopub.status.busy": "2024-11-07T23:13:09.682968Z",
     "iopub.status.idle": "2024-11-07T23:30:21.811653Z",
     "shell.execute_reply": "2024-11-07T23:30:21.810983Z",
     "shell.execute_reply.started": "2024-11-07T23:13:09.683184Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae499481bb05441aaf9350b82bba7101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitud del conjunto de prueba (Random Forest) = 0.4545163868904876\n",
      "Puntaje F1 del conjunto de prueba (Random Forest) = 0.3206838704891863\n",
      "Recall del conjunto de prueba (Random Forest) = 0.4545163868904876"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Crear el modelo Random Forest\n",
    "rf = RandomForestClassifier(featuresCol='tfidf_features', labelCol='tema_predominante', numTrees=50, seed=42)\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Realizar predicciones en los datos de prueba\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Evaluar el modelo usando MulticlassClassificationEvaluator\n",
    "\n",
    "# Exactitud (Accuracy)\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = evaluator_accuracy.evaluate(rf_predictions)\n",
    "print(f\"Exactitud del conjunto de prueba (Random Forest) = {rf_accuracy}\")\n",
    "\n",
    "# Puntaje F1\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "rf_f1 = evaluator_f1.evaluate(rf_predictions)\n",
    "print(f\"Puntaje F1 del conjunto de prueba (Random Forest) = {rf_f1}\")\n",
    "\n",
    "# Recall\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"tema_predominante\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "rf_recall = evaluator_recall.evaluate(rf_predictions)\n",
    "print(f\"Recall del conjunto de prueba (Random Forest) = {rf_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d83b8fd-a06e-491e-b72e-3021256411de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:46.828117Z",
     "iopub.status.busy": "2024-11-07T23:30:46.827885Z",
     "iopub.status.idle": "2024-11-07T23:30:50.097323Z",
     "shell.execute_reply": "2024-11-07T23:30:50.096721Z",
     "shell.execute_reply.started": "2024-11-07T23:30:46.828090Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13090f8fab4f4d96ba8bdcaf6eff7e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## saving\n",
    "# Define la ruta en S3 para almacenar el modelo de Rndom Forest\n",
    "s3_path_rf_model = \"s3://bucketspark14/models/random_forest_final\"\n",
    "\n",
    "# Guardar el modelo de RF en S3\n",
    "rf_model.save(s3_path_rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7984a682-54b4-4d47-b0c5-4216dceafd29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:50.098589Z",
     "iopub.status.busy": "2024-11-07T23:30:50.098427Z",
     "iopub.status.idle": "2024-11-07T23:30:51.360967Z",
     "shell.execute_reply": "2024-11-07T23:30:51.360100Z",
     "shell.execute_reply.started": "2024-11-07T23:30:50.098569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debc4f434d5643c2861b42f081e8f12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## saving\n",
    "# Define la ruta en S3 para almacenar el modelo de Regresión Logística\n",
    "s3_path_lr_model = \"s3://bucketspark14/models/logistic_regression_model_final\"\n",
    "\n",
    "# Guardar el modelo de Regresión Logística en S3\n",
    "lr_model.save(s3_path_lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "241d864a-92cd-42b7-9f86-7b5e024f53c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:51.362789Z",
     "iopub.status.busy": "2024-11-07T23:30:51.362623Z",
     "iopub.status.idle": "2024-11-07T23:30:52.257311Z",
     "shell.execute_reply": "2024-11-07T23:30:52.256662Z",
     "shell.execute_reply.started": "2024-11-07T23:30:51.362769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e393f788d44443aa624d0e14f0f8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## saving\n",
    "# Define la ruta en S3 para almacenar el modelo de DT\n",
    "s3_path_dt_model = \"s3://bucketspark14/models/decision_tree_model_final\"\n",
    "\n",
    "# Guardar el modelo de DT en S3\n",
    "dt_model.save(s3_path_dt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f553747-d59e-4acf-aa92-1ac4fff2cfc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:52.259102Z",
     "iopub.status.busy": "2024-11-07T23:30:52.258861Z",
     "iopub.status.idle": "2024-11-07T23:30:53.523738Z",
     "shell.execute_reply": "2024-11-07T23:30:53.523130Z",
     "shell.execute_reply.started": "2024-11-07T23:30:52.259068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf575b72183b4a80b1fb42cf0db8a705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_path_lda = \"s3://bucketspark14/models/lda_model\"\n",
    "\n",
    "# Guardar el modelo LDA en S3\n",
    "lda_model.save(s3_path_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26d67913-9eac-4fbb-83ce-7a3fb9260138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:53.524905Z",
     "iopub.status.busy": "2024-11-07T23:30:53.524688Z",
     "iopub.status.idle": "2024-11-07T23:30:55.838220Z",
     "shell.execute_reply": "2024-11-07T23:30:55.837464Z",
     "shell.execute_reply.started": "2024-11-07T23:30:53.524874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afb28d8981d4059b57bd6dcdaf29e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_path_cv = \"s3://bucketspark14/models/count_vectorizer_model\"\n",
    "s3_path_idf = \"s3://bucketspark14/models/idf_model\"\n",
    "\n",
    "# Guardar el modelo CountVectorizer en S3\n",
    "cv_model.save(s3_path_cv)\n",
    "\n",
    "# Guardar el modelo IDF en S3\n",
    "idf_model.save(s3_path_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548f8306-1a8b-4317-a63e-79a02e034756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:30:55.839371Z",
     "iopub.status.busy": "2024-11-07T23:30:55.839206Z",
     "iopub.status.idle": "2024-11-07T23:33:07.913328Z",
     "shell.execute_reply": "2024-11-07T23:33:07.912390Z",
     "shell.execute_reply.started": "2024-11-07T23:30:55.839350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b696de32448b43db8800b64ac6a914e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_path_results = \"s3://bucketspark14/results/lda_transformed_data\"\n",
    "\n",
    "# Guardar el DataFrame `df_transformed` en S3 en formato Parquet\n",
    "df_transformed.write.mode(\"overwrite\").parquet(s3_path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33c0d852-cdcf-4cd3-90e9-7aa7ee609ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-07T23:33:07.915856Z",
     "iopub.status.busy": "2024-11-07T23:33:07.915369Z",
     "iopub.status.idle": "2024-11-07T23:35:17.505910Z",
     "shell.execute_reply": "2024-11-07T23:35:17.505120Z",
     "shell.execute_reply.started": "2024-11-07T23:33:07.915820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f0f8a645c4d309a7c7c85e843c21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_path_results = \"s3://bucketspark14/results/cv_transformed_data\"\n",
    "\n",
    "df_tfidf.write.mode(\"overwrite\").parquet(s3_path_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc63264-42f4-4f7a-9330-2c78c93b3970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
